import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
import os
import datetime
from utils.plots import plot_training_metrics

from model.MLP import MLPModel
from utils.data_loader import get_dataloaders
from utils.logger import printlog, save_log

# ******************** ç¯å¢ƒé…ç½® ********************
# è®¾å¤‡é€‰æ‹©ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
printlog(f"Using Device: {device}")


# ******************** å·¥å…·å‡½æ•° ********************
def evaluate(model, data_loader, desc="Testing: "):
    """æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ç‰ˆæœ¬ï¼‰
    åŠŸèƒ½ï¼š
        - åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè®¡ç®—æ¨¡å‹ç²¾åº¦
        - ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
    è¿”å›ï¼š
        - åˆ†ç±»å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰å’ŒæŸå¤±å€¼
    """
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­Dropout/BatchNormï¼‰
    model.eval()
    criterion = nn.CrossEntropyLoss()

    correct = 0
    total = 0
    total_loss = 0

    # æ·»åŠ è¿›åº¦æ¡ï¼Œæ³¨æ„è®¾ç½®æ›´çª„çš„å®½åº¦ä»¥ç¡®ä¿æ˜¾ç¤ºå®Œæ•´
    progress_bar = tqdm(data_loader, desc=desc, ncols=100, leave=False)

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆå‡å°‘å†…å­˜æ¶ˆè€—ï¼‰
    with torch.no_grad():
        for images, labels in progress_bar:
            # æ•°æ®é¢„å¤„ç†
            images = images.view(-1, 28 * 28).to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­è·å–é¢„æµ‹ç»“æœ
            outputs = model(images)
            loss = criterion(outputs, labels)

            # è®¡ç®—æŸå¤±
            total_loss += loss.item() * images.size(0)

            # è·å–é¢„æµ‹ç±»åˆ«
            _, predicted = torch.max(outputs.data, 1)

            # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°
            batch_size = labels.size(0)
            total += batch_size
            batch_correct = (predicted == labels).sum().item()
            correct += batch_correct

            # å®æ—¶æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰å‡†ç¡®ç‡(ç²¾ç®€æ˜¾ç¤º)
            current_acc = 100 * correct / total
            progress_bar.set_postfix({'acc': f'{current_acc:.2f}%'})

    # è¿”å›ç™¾åˆ†æ¯”ç²¾åº¦å’Œå¹³å‡æŸå¤±
    return 100 * correct / total, total_loss / total


# ******************** è®­ç»ƒæµç¨‹ï¼ˆå®Œæ•´å°è£…ï¼‰********************
def main(_config):
    """ä¸»è®­ç»ƒæµç¨‹

    Args:
        _config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°
    """
    # æ•°æ®åŠ è½½
    train_loader, test_loader = get_dataloaders(_config)

    # æ¨¡å‹åˆå§‹åŒ–
    model = MLPModel(input_size=784,
                hidden_size=_config["hidden_dims"],
                num_classes=10,
                layer_num=_config["layer_num"],
                use_dropout=_config['use_dropout'],
                dropout_rate=_config['dropout_rate'],
                use_batch_norm=_config['use_batch_norm']).to(device)

    # æ¨¡å‹å‚æ•°æ•°é‡ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    if total_params >= 1e9:
        printlog(f"Total Parameters: {total_params/1e9:.2f}B")
    elif total_params >= 1e6:
        printlog(f"Total Parameters: {total_params/1e6:.2f}M")
    else:
        printlog(f"Total Parameters: {total_params:,}")

    # ä¼˜åŒ–å™¨é…ç½®(AdamWï¼šå¸¦æƒé‡è¡°å‡ä¿®æ­£çš„Adam)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=_config['learning_rate'],
        weight_decay=_config['weight_decay']  # è§£è€¦æƒé‡è¡°å‡
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨(æ ¹æ®éªŒè¯ç²¾åº¦è°ƒæ•´å­¦ä¹ ç‡)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer=optimizer,
        mode='max',
        factor=0.5,
        patience=2
    ) if _config['use_scheduler'] else None
    
    # è®°å½•åˆå§‹å­¦ä¹ ç‡
    last_lr = _config['learning_rate']

    # æŸå¤±å‡½æ•°(å†…ç½® Softmax)
    criterion = nn.CrossEntropyLoss()

    train_losses = []  # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±
    test_losses = []   # è®°å½•æ¯ä¸ªepochçš„æµ‹è¯•æŸå¤±
    train_acc_list = []  # è®°å½•è®­ç»ƒå‡†ç¡®ç‡
    test_acc_list = [] # è®°å½•æµ‹è¯•å‡†ç¡®ç‡
    best_acc = 0.0     # è®°å½•æœ€ä½³æµ‹è¯•ç²¾åº¦
    
    # ä½¿ç”¨AverageMeterè·Ÿè¸ªè®­ç»ƒæŒ‡æ ‡
    class AverageMeter:
        def __init__(self):
            self.reset()

        def reset(self):
            self.val = 0
            self.avg = 0
            self.sum = 0
            self.count = 0

        def update(self, val, n=1):
            self.val = val
            self.sum += val * n
            self.count += n
            self.avg = self.sum / self.count

    # è®­ç»ƒå¾ªç¯
    for epoch in range(_config['num_epochs']):
        model.train()  # è®­ç»ƒæ¨¡å¼
        train_loss_meter = AverageMeter()
        train_acc_meter = AverageMeter()

        # æ›´æ–°æè¿°ä¿¡æ¯ï¼Œç¡®ä¿è¿›åº¦æ¡å®½åº¦åˆé€‚
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{_config['num_epochs']}", 
                           ncols=100, leave=True)

        for images, labels in progress_bar:
            # æ•°æ®é¢„å¤„ç†
            images = images.view(-1, 28 * 28).to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            # æ›´æ–°å‚æ•°
            optimizer.step()

            # ç»Ÿè®¡è®­ç»ƒæŒ‡æ ‡
            batch_size = labels.size(0)
            _, predicted = torch.max(outputs.data, 1)
            batch_correct = (predicted == labels).sum().item()
            batch_acc = 100 * batch_correct / batch_size

            # æ›´æ–°å¹³å‡æŒ‡æ ‡
            train_loss_meter.update(loss.item(), batch_size)
            train_acc_meter.update(batch_acc, batch_size)

            # åªæ˜¾ç¤ºå…³é”®æŒ‡æ ‡ï¼Œå‡å°‘æ˜¾ç¤ºå®½åº¦
            progress_bar.set_postfix({
                'loss': f'{train_loss_meter.avg:.4f}',
                'acc': f'{train_acc_meter.avg:.2f}%'
            })

        # è®°å½•è®­ç»ƒæŸå¤±
        train_losses.append(train_loss_meter.avg)
        # è®°å½•è®­ç»ƒå‡†ç¡®ç‡
        train_acc_list.append(train_acc_meter.avg)

        # è®¡ç®—æµ‹è¯•æŸå¤±å’Œå‡†ç¡®ç‡ (åªéœ€è¦ä¸€æ¬¡è¯„ä¼°)
        test_acc, test_loss = evaluate(model, test_loader, desc=f"Testing (Epoch {epoch+1})")
        test_acc_list.append(test_acc)
        test_losses.append(test_loss)

        # å­¦ä¹ ç‡è°ƒæ•´ï¼ˆåŸºäºæµ‹è¯•é›†æ€§èƒ½ï¼‰
        if scheduler:
            # æ ¹æ®æµ‹è¯•å‡†ç¡®ç‡è°ƒæ•´å­¦ä¹ ç‡
            scheduler.step(test_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            # ä¿å­˜æ¨¡å‹å‚æ•°
            torch.save(model.state_dict(), "checkpoint/best_model.bin")
            printlog(f"âœ… The model has been saved! New best accuracy: {best_acc:.2f}%")

        # æ‰“å°epochç»“æœ
        epoch_summary = (f"Epoch {epoch + 1}: "
              f"Train Loss: {train_loss_meter.avg:.4f} | "
              f"Train Acc: {train_acc_meter.avg:.2f}% | "
              f"Test Loss: {test_loss:.4f} | "
              f"Test Acc: {test_acc:.2f}% | "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}"
              )

        # è®°å½•åˆ°æ—¥å¿—å¹¶æ‰“å°åˆ°æ§åˆ¶å°
        save_log(epoch_summary)
        print(epoch_summary)

    # è¾“å‡ºæ¨¡å‹æœ€ä½³ç»“æœ
    printlog(f"ğŸ˜ Best Accuracy: {best_acc:.2f}%")

    # è°ƒç”¨ç»˜å›¾å‡½æ•°
    plot_training_metrics(train_losses, test_losses, train_acc_list, test_acc_list)


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è®°å½•è¿è¡Œå¼€å§‹æ—¶é—´
    start_time = datetime.datetime.now()
    printlog(f"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ******************** è¶…å‚æ•°é…ç½®å­—å…¸ ********************
    config = {
        "seed": 42,  # éšæœºç§å­ï¼ˆä¿è¯å¯é‡å¤æ€§ï¼‰
        "batch_size": 512,  # å¤§æ‰¹é‡æå‡è®­ç»ƒé€Ÿåº¦
        "num_epochs": 15,  # é€‚å½“å¢åŠ è®­ç»ƒè½®æ¬¡
        "learning_rate": 1e-3,  # Adamä¼˜åŒ–å™¨çš„å…¸å‹å­¦ä¹ ç‡
        "weight_decay": 1e-4,  # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ç³»æ•°ï¼‰
        "hidden_dims": 256,  # éšè—å±‚ç»´åº¦é…ç½®ï¼ˆå¯çµæ´»è°ƒæ•´ï¼‰
        "dropout_rate": 0.2,  # Dropoutæ¯”ä¾‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        "layer_num": 5,  # MLP å±‚æ•°
        "use_batch_norm": True,  # æ˜¯å¦ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–
        "use_dropout": True,  # æ˜¯å¦ä½¿ç”¨Dropout
        "use_scheduler": True  # æ˜¯å¦å¯ç”¨å­¦ä¹ ç‡è°ƒåº¦
    }
    printlog(config)
    main(config)  # æ‰§è¡Œè®­ç»ƒæµç¨‹
    
    # è®°å½•è¿è¡Œç»“æŸæ—¶é—´
    end_time = datetime.datetime.now()
    duration = end_time - start_time
    printlog(f"Training completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    printlog(f"Total training time: {duration.total_seconds()/60:.2f} minutes")
